# -*- coding: utf-8 -*-
"""ML-HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kypvzlQsla0Uzp3Q5BjN43DPrpQ_7v3J
"""

import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data
from sklearn.preprocessing import StandardScaler
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torchvision import models
from sklearn import metrics
from sklearn import decomposition
from sklearn import manifold
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image

"""### **Q3**"""

class Featuredata(data.Dataset):
  def __init__(self, file):
    df=pd.read_csv(file,header=None)
    x=df.iloc[:,1:]
    y=df.iloc[:,0]
    sc=StandardScaler()
    x=sc.fit_transform(x)
    self.X_train=torch.tensor(x,dtype=torch.float32)
    self.y_train=y
  def __len__(self):
    return len(self.y_train)
  def __getitem__(self,index):
    return self.X_train[index],self.y_train[index]
traindata=Featuredata('/content/drive/MyDrive/ML DataSet/largeTrain.csv')
valdata=Featuredata('/content/drive/MyDrive/ML DataSet/largeValidation.csv')
trainloader = data.DataLoader(traindata, batch_size=64, shuffle=True)
valloader=data.DataLoader(valdata,batch_size=64, shuffle=True)

"""**PART A**"""

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

hidden_size=[5,20,50,100,200] #given hidden layer sizes

criterion = nn.CrossEntropyLoss()
loss_size=np.zeros(len(hidden_size))
loss_val=np.zeros(len(hidden_size))
for s in range(len(hidden_size)):
  n=hidden_size[s]
  model=nn.Sequential(nn.Linear(128,n),nn.ReLU(),nn.Linear(n,10),nn.Softmax(dim=1))
  model.apply(init_weights)
  optimizer = optim.SGD(model.parameters(), lr=0.01)
  epochs=100
  loss_n=0.0
  for e in range(epochs):
    running_loss=0.0
    for x,y in trainloader:
      optimizer.zero_grad()
      output=model(x)
      loss=criterion(output,y)
      loss.backward()
      optimizer.step()
      running_loss+=loss.item()
    loss_n+=running_loss/len(trainloader)
  loss_n/=epochs
  loss_size[s]=loss_n
  valloss=0.0
  with torch.no_grad():
    for x,y in valloader:
      output=model(x)
      loss=criterion(output,y)
      valloss+=loss
  loss_val[s]=valloss/len(valloader)

plt.plot(hidden_size,loss_size,'b-',label='train cross entropy')
plt.plot(hidden_size,loss_val,'r-',label='val cross entropy')
plt.legend()

"""**PART B**"""

lrates=[0.1,0.01,0.001]
for s in range(len(lrates)):
  plt.figure(figsize=(10,7))
  
  criterion = nn.CrossEntropyLoss()
  model=nn.Sequential(nn.Linear(128,64),nn.ReLU(),nn.Linear(64,10),nn.Softmax())
  model.apply(init_weights)
  optimizer = optim.SGD(model.parameters(), lr=lrates[s])
  epochs=100
  loss_n=0.0
  loss_size=np.zeros(epochs)
  loss_val=np.zeros(epochs) 
  for e in range(epochs):
    running_loss=0.0
    for x,y in trainloader:
      optimizer.zero_grad()
      output=model(x)
      loss=criterion(output,y)
      loss.backward()
      optimizer.step()
      running_loss+=loss.item()
    loss_n+=running_loss/len(trainloader)
    loss_size[e]=loss_n/(e+1)
    valloss=0.0
    with torch.no_grad():
      for x,y in valloader:
        output=model(x)
        loss=criterion(output,y)
        valloss+=loss
    loss_val[e]=valloss/len(valloader)
  plt.plot(np.arange(1,101),loss_size,'b-',label='train cross entropy')
  plt.plot(np.arange(1,101),loss_val,'r-',label='val cross entropy')
  plt.legend()

"""#**Q4**"""

df=pd.read_pickle('/content/drive/MyDrive/ML DataSet/train_CIFAR.pickle',compression=None)

X=df['X']
y=df['Y']
X.shape

print("----Exploratory Data Analysis----")
print('# of Samples', X.shape[0])
print('maximum pixel value',np.max(df['X']))
print('minimum pixel value',np.min(df['X']))
print('mean pixel value',np.mean(df['X'].mean(axis=0)))
print('class distributions',np.unique(y,return_counts=True))
print('for class 0')
X_0=X[y==0]
print('mean pixel value',np.mean(X_0.mean(axis=0)))
print('max pixel value',np.max(X_0))
print('min pixel value',np.min(X_0))
print('for class 1')
X_1=X[y==1]
print('mean pixel value',np.mean(X_1.mean(axis=0)))
print('max pixel value',np.max(X_1))
print('min pixel value',np.min(X_1))
print('first 10 labels',y[:10])
print('---          The END           ---')

images=df['X'].reshape((nimg,3,32,32))[:10]
images.shape
n_images = len(images)
rows = int(np.sqrt(n_images))
cols = int(np.sqrt(n_images))

fig = plt.figure(figsize = (10, 10))

for i in range(rows*cols):

    ax = fig.add_subplot(rows, cols, i+1)
    
    image = images[i]
    ax.imshow(image.reshape(32,32,3))
    ax.axis('off')

"""##**Part B**"""

alexnet=models.alexnet(pretrained=True)

# x=transforms.functional.to_tensor(X.to_numpy())
nimg=len(X)

x=X.to_numpy().reshape((nimg,3,32,32))
# x=x.view(-1,32,32,3)
# x=x.permute(0,3,1,2)
conv = transforms.Compose([transforms.Resize((224,224)),
                           transforms.ToTensor(), 
                           transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])
features = []

for i in range(nimg):
    pilimg = Image.fromarray(np.array(x[i].T))
    tensor = conv(pilimg).unsqueeze(0)
    fc_out = alexnet(tensor)
#     feature = fc_out.detach().numpy()
#     feature = numpy.squeeze(feature)
    feature = np.squeeze(fc_out).tolist()
    features.append(feature)

from sklearn.preprocessing import normalize
features=normalize(features)
features.shape

output=output.detach().numpy()



df=pd.read_pickle('/content/drive/MyDrive/ML DataSet/test_CIFAR.pickle')
nimgt=len(df['X'])

x_test=df['X'].reshape((nimgt,3,32,32))
# x=x.view(-1,32,32,3)
# x=x.permute(0,3,1,2)
conv = transforms.Compose([transforms.Resize((224,224)),
                           transforms.ToTensor(), 
                           transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])
features_test = []

for i in range(nimgt):
    pilimg = Image.fromarray(np.array(x_test[i].T))
    tensor = conv(pilimg).unsqueeze(0)
    fc_out = alexnet(tensor)
#     feature = fc_out.detach().numpy()
#     feature = numpy.squeeze(feature)
    feature = np.squeeze(fc_out).tolist()
    features_test.append(feature)

features_test=normalize(features_test)

from sklearn.neural_network import MLPClassifier
clf=MLPClassifier(hidden_layer_sizes=(512,256))
clf.fit(features,y)

from sklearn.metrics import accuracy_score, confusion_matrix,roc_curve
y_pred=clf.predict(features_test)
df=pd.read_pickle('/content/drive/MyDrive/ML DataSet/test_CIFAR.pickle')
y_test=df['Y']
# accuracy_score(y_pred,y_test)

print(accuracy_score(y_pred,y_test))
import seaborn as sns
sns.heatmap(confusion_matrix(y_test,y_pred))

prob=clf.predict_proba(features_test)

fpr,tpr,threshold=roc_curve(y_test,prob[:,1])

plt.plot(fpr,tpr,marker='.')

